<H3>A few thoughts on software engineering principles for government analysis...</H3>

<p>I am very keen to promote principles of clean, quality assured code as espoused in the Government Analytical Function's <a href="https://best-practice-and-impact.github.io/qa-of-code-guidance/intro.html"> quality assurance framework. </a> </p>

<p>In practice, this means promoting:</p>
<li>Proportionate usage of unit testing (e.g. automated testing to run on PRs via GitHub Actions)</li>
<li>Modular codebase design, considered from day one of any analytical project.</li>
<li>Coding standards, including e.g. the usage of an agreed style guide and the implementation of linting to enforce this.</li>

<p>It struck me that these principles are basic approaches software engineers would insist upon to keep their codebases maintainable and well-tested. 
    
<p>And yet in many departments' transitions to automated analysis, these principles are not yet universal. I believe as more and more analysis moves into R and Python for the purpose of automation, there is a lot we need to learn from software engineers. The risk of failing to do so would be wasted taxpayers' money funding a merry-go-round of analysts re-building the same tools again and again. Not a huge problem (and perhaps not proportionate) for one-off ad hoc analysis, but something we <i>have</i> to avoid for long term analytical products.</p>

<p>Principles from books such as 'The Pragmatic Programmer', 'Clean Code', and 'Code that Fits In Your Head' should not only be the preserve of our colleagues in DDaT roles. This is why I am currently reading through such books, following software developer chatter about maintainability, and looking to see where I can integrate these good practices into the day-to-day work my team carries out. My longer term mission is to promote these more broadly within Government.</p>

<p>I am still very much learning, so by no means do I claim to be an expert on this!</p>


